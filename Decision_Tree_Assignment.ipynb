{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkqVtnbU38Ju"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Assignment\n",
        "\n",
        "1.  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        " - A Decision Tree is a popular supervised machine learning algorithm used for classification and regression tasks. In the context of classification, it helps to predict the category or class of an input sample based on its features.\n",
        "\n",
        "How does it work in the context of classification -\n",
        "\n",
        " - Feature Selection and Splitting:\n",
        "\n",
        "The algorithm selects the best feature to split the data based on a criterion like Gini impurity, Entropy, or Information Gain.\n",
        "\n",
        "It recursively splits the dataset into subsets where each subset becomes a new node.\n",
        "\n",
        "- Recursive Partitioning:\n",
        "\n",
        "This process continues until one of the stopping conditions is met:\n",
        "\n",
        "All instances in a node belong to the same class.\n",
        "\n",
        "No features are left to split.\n",
        "\n",
        "A maximum tree depth is reached.\n",
        "\n",
        "- Prediction:\n",
        "\n",
        "To classify a new instance, start at the root and follow the decisions down the tree until a leaf node is reached, which gives the predicted class.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        " - Both Gini Impurity and Entropy are metrics used to evaluate how \"pure\" or \"impure\" a node is during the process of splitting in a decision tree. The goal is to select the feature and threshold that results in the most homogeneous (pure) child nodes.\n",
        "\n",
        " At each node, the algorithm evaluates all possible splits and selects the one that produces the largest reduction in impurity (called Information Gain for Entropy or Gini Gain for Gini Impurity).\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        " -  Pre-Pruning (Early Stopping)\n",
        "\n",
        "Definition :\n",
        "Pre-pruning involves stopping the tree growth early, i.e., before it becomes overly complex. The decision to stop is made during the tree construction process.\n",
        "\n",
        "- Common Pre-Pruning Criteria:\n",
        "\n",
        "Maximum tree depth (max_depth)\n",
        "\n",
        "Minimum samples required to split a node (min_samples_split)\n",
        "\n",
        "Minimum samples per leaf (min_samples_leaf)\n",
        "\n",
        "Minimum information gain or Gini gain threshold\n",
        "\n",
        " - Post-Pruning (Cost-Complexity Pruning)\n",
        "\n",
        "Definition :\n",
        "Post-pruning allows the tree to grow fully first, and then prunes back branches that have little importance or contribute to overfitting.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Reduced Error Pruning\n",
        "\n",
        "Cost Complexity Pruning (used in CART, based on complexity parameter α)\n",
        "\n",
        "Validation set pruning (prune if validation accuracy improves)\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        " - Information Gain (IG) is a measure of how well a particular feature separates the data into target classes. It quantifies the reduction in entropy (or disorder) after a dataset is split based on a feature.\n",
        "\n",
        " Why is it Important?\n",
        "\n",
        " In decision trees (especially algorithms like ID3 and C4.5), Information Gain is the key criterion used to choose the best feature to split a node. A higher information gain indicates a more effective feature for creating pure child nodes.\n",
        "\n",
        " Choosing the feature with the highest IG at each node leads to a tree that classifies data more accurately.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        " - Real-World Applications\n",
        "\n",
        "- Healthcare\n",
        "\n",
        "Use: Diagnosing diseases based on symptoms and test results\n",
        "\n",
        "Example: Predicting whether a patient has diabetes based on features like glucose level, BMI, age\n",
        "\n",
        "- Finance & Banking\n",
        "\n",
        "Use: Credit scoring, fraud detection\n",
        "\n",
        "Example: Deciding whether to approve a loan based on income, credit history, and debt ratio\n",
        "\n",
        "- Marketing\n",
        "\n",
        "Use: Customer segmentation, churn prediction\n",
        "\n",
        "Example: Predicting if a customer will respond to a marketing campaign\n",
        "\n",
        "- Retail\n",
        "\n",
        "Use: Recommender systems, inventory decisions\n",
        "\n",
        "Example: Predicting product preferences or seasonal demand\n",
        "\n",
        "- Education\n",
        "\n",
        "Use: Student performance prediction\n",
        "\n",
        "Example: Predicting dropouts based on attendance, grades, and engagement\n",
        "\n",
        "- Manufacturing\n",
        "\n",
        "Use: Quality control and defect detection\n",
        "\n",
        "Example: Classifying whether a product is defective based on sensor readings\n",
        "\n",
        "- Main Advantages\n",
        "\n",
        "     Advantage\t                                 Explanation\n",
        "\n",
        "Easy to Understand\t    Tree structure is intuitive and can be visualized easily\n",
        "\n",
        "No Need for Feature Scaling\t       Works well with unscaled or categorical data\n",
        "\n",
        "Handles Non-linear Relationships\t   Captures complex interactions between     \n",
        "features\n",
        "\n",
        "Fast to Train and Predict\t       Computationally efficient for small to medium datasets\n",
        "\n",
        "Interpretable\t                Each decision path provides a clear explanation of the prediction process\n",
        "\n",
        "6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "- Load the Iris Dataset\n"
      ],
      "metadata": {
        "id": "ns3xtID54J39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "df['target_name'] = df['target'].apply(lambda x: iris.target_names[x])\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "2RfRfReL9mNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train a Decision Tree Classifier using the Gini criterion"
      ],
      "metadata": {
        "id": "-vezYOoM9rZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree classifier using Gini index\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "fs1DLRX19uTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "fkkK6kaP96ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "importances = clf.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "SdKSbfLU9_fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "- Load the Iris Dataset"
      ],
      "metadata": {
        "id": "yT_ABPyV-LHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "\n",
        "# Add the target column (numerical labels)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Add the target names (class labels)\n",
        "df['target_name'] = df['target'].apply(lambda i: iris.target_names[i])\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "9DtXrZxg-UWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "SDBWny-U-dev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully-grown Decision Tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Output accuracies\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "GSDmDKJR-k6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "- Load the Boston Housing Dataset"
      ],
      "metadata": {
        "id": "se7qmSqq-uj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Boston dataset\n",
        "boston = load_boston()\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "df['MEDV'] = boston.target  # MEDV = Median value of owner-occupied homes\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "3iVYw6mp-1Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train a Decision Tree Regressor"
      ],
      "metadata": {
        "id": "8mqJEqKz_Br1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load Boston Housing dataset from statsmodels\n",
        "boston = sm.datasets.get_rdataset(\"Boston\", \"MASS\").data\n",
        "\n",
        "# Features and target\n",
        "X = boston.drop(\"medv\", axis=1)\n",
        "y = boston[\"medv\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion='squared_error', random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "5N3oXs8t_HQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "FBjKCsgK_Plm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = sm.datasets.get_rdataset(\"Boston\", \"MASS\").data\n",
        "\n",
        "# Features and target\n",
        "X = boston.drop(\"medv\", axis=1)\n",
        "y = boston[\"medv\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion='squared_error', random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print Feature Importances\n",
        "importances = regressor.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "8DOcuOb-_TbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "- Load the Iris Dataset"
      ],
      "metadata": {
        "id": "a33BPscV_bN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Create a DataFrame with feature data\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "\n",
        "# Add the target labels (numerical)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Map numerical target labels to class names (e.g., setosa, versicolor, virginica)\n",
        "df['target_name'] = df['target'].apply(lambda x: iris.target_names[x])\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "GXEMh6_Q_izA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV"
      ],
      "metadata": {
        "id": "bQo7hdCx_sUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters Found:\", grid_search.best_params_)\n",
        "\n",
        "# Best model evaluation\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on Test Set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "x1BxOaFU_y-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "68ysLd7W_-29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters Found:\", best_params)\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "1NCZlERN__7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "- Step-by-Step Workflow\n",
        " - Handle Missing Values\n",
        "Goal: Clean the dataset to ensure model compatibility and integrity.\n",
        "\n",
        "Numerical features:\n",
        "\n",
        "Use mean or median imputation:"
      ],
      "metadata": {
        "id": "Jgc8klbxAJyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X_num = num_imputer.fit_transform(X[numeric_columns])\n"
      ],
      "metadata": {
        "id": "SmXKlYPpAmLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Encode Categorical Features\n",
        "\n",
        " Goal: Convert categorical features into numerical form.\n",
        "\n",
        "Low-cardinality categorical features (e.g., gender, smoker):\n",
        "\n",
        "Use One-Hot Encoding:"
      ],
      "metadata": {
        "id": "4-RYyfP0ApuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "X_cat_encoded = encoder.fit_transform(X_cat)\n"
      ],
      "metadata": {
        "id": "Ld8OBmD1AwQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train a Decision Tree Model\n",
        "\n",
        "Goal: Fit a model to the cleaned, encoded data."
      ],
      "metadata": {
        "id": "rlN3pGAvAyQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine numerical and encoded categorical features\n",
        "import numpy as np\n",
        "X_combined = np.hstack((X_num, X_cat_encoded))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train model\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "qAQkfE33A2qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tune Hyperparameters\n",
        "\n",
        "Goal: Optimize model for better generalization using GridSearchCV."
      ],
      "metadata": {
        "id": "Dw5O5XfSA4wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "id": "CjjiJD35A8oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Evaluate Model Performance\n",
        "\n",
        "Goal: Assess how well the model predicts disease status."
      ],
      "metadata": {
        "id": "N6gSzilzA-3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# If using probabilities:\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "id": "kFWpjhmuBDYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}